{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14606b41-8adf-4e96-9d73-e9246b140f3f",
   "metadata": {},
   "source": [
    "## Exteval Modified Corrector Research\n",
    "\n",
    "This notebooks contains my research regarding the EXTEVAL-Modified-Metric.\n",
    "The metric was modified to output more information of the detected errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abd1eed-ad0f-4f26-ae30-b50020a6a9ad",
   "metadata": {},
   "source": [
    "### Step 1: Run ExtEval-Modified on the summaries to see the exact scores it predicts for `data_only_incorrect.json` witd extended information for the found errors\n",
    "\n",
    "These scores will be saved in `data_only_incorrect_exteval_modified.json` and are needed for the prompt to `GPT-4o` and later for the `Evaluation`.\n",
    "The new exteval modified will provide a lot more information for `GPT-4o mini` and will ensure a better correction later.\n",
    "\n",
    "(The `data_only_incorrect.json` was already created by `Step 1` of the `exteval-corrector-research`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19ed16f8-a24a-4ac7-8ba6-1d65ce5bb999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command executed successfully.\n",
      "837.0383448600769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define the command and its arguments\n",
    "command = [\n",
    "    \"python3\",\n",
    "    \"exteval/extevalModified.py\",\n",
    "    \"--data_file\", \"data/data_only_incorrect.json\",\n",
    "    \"--output_file\", \"data/scores/data_only_incorrect_exteval_modified.json\"\n",
    "]\n",
    "\n",
    "# Execute the command\n",
    "result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "# Print the output and errors\n",
    "if result.returncode == 0:\n",
    "    print(\"Command executed successfully.\")\n",
    "    print(result.stdout)\n",
    "else:\n",
    "    print(\"Error occurred while executing the command.\")\n",
    "    print(result.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc088214-b737-4a08-b4df-85f14c694e80",
   "metadata": {},
   "source": [
    "### Step 2: Combine incorrect data (`data_only_incorrect.json`) with their exteval scores (`data_only_incorrect_exteval_modified.json`) -> Prepartion for easy access in the prompt\n",
    "\n",
    "The new merged data will be saved in `data_incorrect_merged_modified.json` and will be used for the prompt to `GPT-4o` in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1965d623-8b71-4754-bf30-ff3726238e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged JSON saved to data/merged/data_only_incorrect_merged_modified.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "raw_data = \"data/data_only_incorrect.json\"\n",
    "exteval_modified_data = \"data/scores/data_only_incorrect_exteval_modified.json\"\n",
    "output_path = \"data/merged/data_only_incorrect_merged_modified.json\"\n",
    "\n",
    "# read_data\n",
    "with open(raw_data, 'r', encoding='utf-8') as f1:\n",
    "    data1 = json.load(f1)\n",
    "\n",
    "with open(exteval_modified_data, 'r', encoding='utf-8') as f2:\n",
    "    data2 = json.load(f2)\n",
    "\n",
    "# result dictionary for the new json\n",
    "merged_data = {}\n",
    "\n",
    "# Merging of the data\n",
    "for key, value2 in data2.items():\n",
    "    if key in data1:  # check if the key is in both files\n",
    "        value1 = data1[key]\n",
    "        \n",
    "        # create new structure\n",
    "        merged_entry = {\n",
    "            **value2, # gets all fields from the second json\n",
    "            \"summary\": value1.get(\"summary\"),\n",
    "            \"document\": value1.get(\"document\"),\n",
    "            \"summary_for_annotation\": value1.get(\"summary_for_annotation\"),\n",
    "            \"document_for_annotation\": value1.get(\"document_for_annotation\")\n",
    "        }\n",
    "        \n",
    "        # save inside new json\n",
    "        merged_data[key] = merged_entry\n",
    "\n",
    "# save completed new json\n",
    "with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(merged_data, output_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Merged JSON saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ba1b36-0a3d-4a1b-bfc2-2a9921805bc5",
   "metadata": {},
   "source": [
    "### Step 3: Call `GPT-4o` for every entry inside the `data_only_incorrect_merged_modified.json` file with the prompt mask with more information from ExtEval-Modified\n",
    "\n",
    "For every entry (484 entries) a call to `GPT-4o` will be made to get a new extractive summary which will be saved in a new file called `data_new.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a000b33e-ea5f-4290-8a01-e0297230f034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONDecodeError for 207_textrank_st: Expecting ',' delimiter: line 6 column 138 (char 849)\n",
      "Response Text: ```json\n",
      "{\n",
      "    \"corrected_extractive_summary\": \"<t> The family respectfully requested that people do not vote for Hillary Clinton in 2016, as stated in Larry Upright's obituary. </t> <t> Marina Shear of Dallas wrote in the online guestbook, 'You have my solemn promise I will not waste a vote on Hillary Clinton.'</t>\",\n",
      "    \"justifications\": {\n",
      "        \"IncorCorefEval\": \"No instances of incorrect coreference needed addressing.\",\n",
      "        \"IncomCorefEval\": \"Clarified 'the family' in the first summary sentence by specifying 'The family of Larry Upright' to provide context. Clarified 'the obituary's' in the second summary sentence by explicitly mentioning 'Larry Upright's obituary' to address incomplete antecedents.\",\n",
      "        \"IncomDiscoEval\": \"Removed the discourse marker 'also' and rewrote the sentence to eliminate the misleading progression implied.\"),\n",
      "        \"SentiBias\": \"Adjusted the first sentence to more neutrally state the family's request and the second sentence's expression of personal sentiment by Marina Shear, avoiding amplification of negative sentiment and aligning reading sentiment with the original document's overall tone.\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "Results were saved to: data/corrected/corrected_data_modified.json.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import re\n",
    "\n",
    "client = OpenAI(api_key=\"<your_api_key>\")\n",
    "\n",
    "# Create the prompt\n",
    "prompt = \"\"\"\n",
    "You are an expert AI assistant specializing in extractive summarization and evaluation using an advanced EXTEVAL framework. EXTEVAL assesses extractive summaries' faithfulness using the following error categories and metrics:\n",
    "\n",
    "### EXTEVAL Metrics\n",
    "1. **Incorrect Coreference (INCORCOREFEVAL):**\n",
    "   - **Definition:** Refers to incorrect mapping of pronouns or noun phrases to their antecedents.\n",
    "   - **Details Provided:** Count and list of specific instances, including:\n",
    "     - Problematic sentence(s)\n",
    "     - Specific mention(s)\n",
    "     - Error type (e.g., \"ambiguous pronoun\").\n",
    "\n",
    "2. **Incomplete Coreference (INCOMCOREFEVAL):**\n",
    "   - **Definition:** Indicates missing or unclear antecedents for references in the text.\n",
    "   - **Details Provided:** Count and list of specific instances, including:\n",
    "     - Problematic sentence(s)\n",
    "     - Specific mention(s)\n",
    "     - Error type (e.g., \"missing antecedent\").\n",
    "\n",
    "3. **Incorrect Discourse (INCOMDISCOEVAL):**\n",
    "   - **Definition:** Highlights faulty or misleading discourse relationships, such as inappropriate use of conjunctions or connectors.\n",
    "   - **Details Provided:** Count and list of specific instances, including:\n",
    "     - Problematic sentence(s)\n",
    "     - Specific discourse marker(s).\n",
    "\n",
    "4. **Sentiment Bias (SENTIBIAS):**\n",
    "   - **Definition:** Measures misalignment of sentiments between the source document and the summary.\n",
    "   - **Details Provided:**\n",
    "     - Absolute difference between document and summary sentiment.\n",
    "     - Average sentiment of the source document and summary.\n",
    "     - List of significant deviations with:\n",
    "       - Problematic sentence(s)\n",
    "       - Document sentiment\n",
    "       - Summary sentiment.\n",
    "\n",
    "5. **Overall EXTEVAL Score (EXTEVAL):**\n",
    "   - Represents a weighted aggregation of all sub-metrics, with higher values indicating greater issues.\n",
    "\n",
    "### Task\n",
    "I will provide:\n",
    "- The **original document**.\n",
    "- The **extractive summary**.\n",
    "- Detailed EXTEVAL scores with specific error details.\n",
    "\n",
    "Your responsibilities:\n",
    "1. **Analyze** the provided EXTEVAL scores and error details to identify the problematic areas in the summary.\n",
    "2. **Revise** the summary to address the identified issues, ensuring it is faithful, coherent, and sentimentally aligned with the source document.\n",
    "\n",
    "### Response Format\n",
    "Respond in this JSON format:\n",
    "```json\n",
    "{{\n",
    "    \"corrected_extractive_summary\": \"<your revised summary>\",\n",
    "    \"justifications\": {{\n",
    "        \"IncorCorefEval\": \"<summary of changes made>\",\n",
    "        \"IncomCorefEval\": \"<summary of changes made>\",\n",
    "        \"IncomDiscoEval\": \"<summary of changes made>\",\n",
    "        \"SentiBias\": \"<summary of changes made>\"\n",
    "    }}\n",
    "}}\n",
    "```\n",
    "\n",
    "### Inputs\n",
    "Here is the original document: {original_text}\n",
    "Here is the extractive summary: {extractive_summary}\n",
    "Here are the EXTEVAL scores: {exteval_scores}\n",
    "\n",
    "### Notes\n",
    "For Incorrect Coreference and Incomplete Coreference, revise pronouns or noun phrases to ensure accurate and clear references.\n",
    "For Incorrect Discourse, restructure sentences or replace discourse markers to create logical and coherent relationships.\n",
    "For Sentiment Bias, adjust phrasing or tone to align the summary's sentiment with the source document's sentiment distribution. \"\"\"\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def save_json(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "# function for the prompts\n",
    "def generate_prompts_and_save(input_file, output_file):\n",
    "    data = load_json(input_file)\n",
    "    output_data = {}\n",
    "\n",
    "    for key, entry in data.items():\n",
    "        document = entry.get(\"document\", \"\")\n",
    "        summary = entry.get(\"summary\", \"\")\n",
    "        exteval_scores = {\n",
    "            \"IncorCorefEval\": entry.get(\"IncorCorefEval\", \"\"),\n",
    "            \"IncomCorefEval\": entry.get(\"IncomCorefEval\", \"\"),\n",
    "            \"IncomDiscoEval\": entry.get(\"IncomDiscoEval\", \"\"),\n",
    "            \"SentiBias\": entry.get(\"SentiBias\", \"\"),\n",
    "            \"ExtEval\": entry.get(\"ExtEval\", \"\"),\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # api call\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a highly knowledgeable assistant trained in extractive summarization and evaluation.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt.format(\n",
    "                        original_text=document,\n",
    "                        extractive_summary=summary,\n",
    "                        exteval_scores=exteval_scores\n",
    "                    )}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # access the answer\n",
    "            response_text = response.choices[0].message.content\n",
    "\n",
    "            #extract only the json part from the answer\n",
    "            json_match = re.search(r\"{.*}\", response_text, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_text = json_match.group(0)\n",
    "                try:\n",
    "                    response_json = json.loads(json_text) \n",
    "                    corrected_extractive_summary = response_json.get(\"corrected_extractive_summary\", None)\n",
    "                    justifications = response_json.get(\"justifications\", None)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"JSONDecodeError for {key}: {e}\")\n",
    "                    print(f\"Response Text: {response_text}\")\n",
    "                    corrected_extractive_summary = None\n",
    "                    justifications = None\n",
    "            else:\n",
    "                print(f\"No valid JSON found in response for {key}. Response Text: {response_text}\")\n",
    "                corrected_extractive_summary = None\n",
    "                justifications = None\n",
    "\n",
    "            # save the results\n",
    "            output_data[key] = {\n",
    "                \"document\": document,\n",
    "                \"corrected_extractive_summary\": corrected_extractive_summary,\n",
    "                \"justifications\": justifications\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error while trying to get a response from GPT-4 {key}: {e}\")\n",
    "            output_data[key] = {\n",
    "                \"document\": document,\n",
    "                \"response\": f\"Error: {e}\"\n",
    "            }\n",
    "\n",
    "    # save all results to a json\n",
    "    save_json(output_data, output_file)\n",
    "    print(f\"Results were saved to: {output_file}.\")\n",
    "\n",
    "\n",
    "# Call\n",
    "input_file_path = \"data/merged/data_only_incorrect_merged_modified.json\"\n",
    "output_file_path = \"data/corrected/corrected_data_modified.json\"\n",
    "generate_prompts_and_save(input_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c644bed2-5c68-4d76-b908-c34f9e3e80e9",
   "metadata": {},
   "source": [
    "### Key Points from the prompt:\n",
    "\n",
    "1. **Enhanced EXTEVAL Details**: Incorporated the extended EXTEVAL scores with **count, details, sentiment deviations**, and metrics for better analysis.\n",
    "2. **Actionable Justifications**: Added more context in the \"justifications\" section for each metric.\n",
    "3. **Clarity in Sentiment Bias**: Highlighted specific sentences with sentiment deviations, providing sentence-level sentiment values to aid targeted corrections.\n",
    "4. **Structured Error Insights**: Explicitly included `summary_sentence`, `mention`, `type`, and `discourse_marker` in the task instructions, helping the AI pinpoint corrections.\n",
    "\n",
    "This refined prompt ensures detailed insights for correction while leveraging all the nuances in the new EXTEVAL data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6dc7e7-66dd-448a-be0e-cfad23326d34",
   "metadata": {},
   "source": [
    "### Error Handling\n",
    "\n",
    "The prompting to `GPT-4o` throwed one errors. For this one errors the prompt was manually created and given to `GPT-4o` to correct. The response was inserted into the `corrected_data_modified.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed9f5de-72f5-422e-ab86-bdcaa3216e80",
   "metadata": {},
   "source": [
    "### Step 4: Now the new and corrected summaries are evaluated by ExtEval Modified to get new scores (for the summaries that were corrected with the extra information from ExtEval Modified)\n",
    "\n",
    "For this the new summaries need to be processed (`preprocess.py`) and after that evaluated (`exteval_modified.py`). The new scores for the corrected summaries will be saved to `data_new_exteval_modified.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7db51291-ee8a-498c-8306-e7276e4ebe17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while executing the script:\n",
      "/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "Some weights of BertModel were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "  0%|          | 0/484 [00:00<?, ?it/s]/stage/allennlp/allennlp/modules/token_embedders/pretrained_transformer_embedder.py:385: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  num_effective_segments = (seq_lengths + self._max_length - 1) // self._max_length\n",
      "\n",
      "  0%|          | 1/484 [00:04<37:46,  4.69s/it]\n",
      "  0%|          | 2/484 [00:09<35:54,  4.47s/it]\n",
      "  1%|          | 3/484 [00:13<37:08,  4.63s/it]\n",
      "  1%|          | 4/484 [00:18<37:48,  4.73s/it]\n",
      "  1%|          | 5/484 [00:23<37:16,  4.67s/it]\n",
      "  1%|          | 6/484 [00:28<38:28,  4.83s/it]\n",
      "  1%|▏         | 7/484 [00:33<38:31,  4.85s/it]\n",
      "  2%|▏         | 8/484 [00:37<38:01,  4.79s/it]\n",
      "  2%|▏         | 9/484 [00:42<36:56,  4.67s/it]\n",
      "  2%|▏         | 10/484 [00:46<36:16,  4.59s/it]\n",
      "  2%|▏         | 11/484 [00:51<37:04,  4.70s/it]\n",
      "  2%|▏         | 12/484 [00:56<36:42,  4.67s/it]\n",
      "  3%|▎         | 13/484 [01:00<36:29,  4.65s/it]\n",
      "  3%|▎         | 14/484 [01:05<35:26,  4.53s/it]\n",
      "  3%|▎         | 15/484 [01:09<35:37,  4.56s/it]\n",
      "  3%|▎         | 16/484 [01:14<35:19,  4.53s/it]\n",
      "  4%|▎         | 17/484 [01:18<35:22,  4.55s/it]\n",
      "  4%|▎         | 18/484 [01:21<31:52,  4.10s/it]\n",
      "  4%|▍         | 19/484 [01:24<28:16,  3.65s/it]\n",
      "  4%|▍         | 20/484 [01:27<26:25,  3.42s/it]\n",
      "  4%|▍         | 21/484 [01:30<25:35,  3.32s/it]\n",
      "  5%|▍         | 22/484 [01:33<24:19,  3.16s/it]\n",
      "  5%|▍         | 23/484 [01:36<23:59,  3.12s/it]\n",
      "  5%|▍         | 24/484 [01:41<29:13,  3.81s/it]\n",
      "  5%|▌         | 25/484 [01:47<33:08,  4.33s/it]\n",
      "  5%|▌         | 26/484 [01:51<33:29,  4.39s/it]\n",
      "  6%|▌         | 27/484 [01:56<34:41,  4.55s/it]\n",
      "  6%|▌         | 28/484 [02:01<34:44,  4.57s/it]\n",
      "  6%|▌         | 29/484 [02:04<31:33,  4.16s/it]\n",
      "  6%|▌         | 30/484 [02:07<28:48,  3.81s/it]\n",
      "  6%|▋         | 31/484 [02:10<27:08,  3.59s/it]\n",
      "  7%|▋         | 32/484 [02:15<29:23,  3.90s/it]\n",
      "  7%|▋         | 33/484 [02:20<31:52,  4.24s/it]\n",
      "  7%|▋         | 34/484 [02:25<34:42,  4.63s/it]\n",
      "  7%|▋         | 35/484 [02:30<34:49,  4.65s/it]\n",
      "  7%|▋         | 36/484 [02:32<29:38,  3.97s/it]\n",
      "  8%|▊         | 37/484 [02:35<25:34,  3.43s/it]\n",
      "  8%|▊         | 38/484 [02:37<23:20,  3.14s/it]\n",
      "  8%|▊         | 39/484 [02:39<21:11,  2.86s/it]\n",
      "  8%|▊         | 40/484 [02:42<20:02,  2.71s/it]\n",
      "  8%|▊         | 41/484 [02:44<19:52,  2.69s/it]\n",
      "  9%|▊         | 42/484 [02:47<18:58,  2.58s/it]\n",
      "  9%|▉         | 43/484 [02:49<18:55,  2.58s/it]\n",
      "  9%|▉         | 44/484 [02:52<19:10,  2.61s/it]\n",
      "  9%|▉         | 45/484 [02:56<23:27,  3.21s/it]\n",
      " 10%|▉         | 46/484 [03:00<23:18,  3.19s/it]\n",
      " 10%|▉         | 47/484 [03:01<19:40,  2.70s/it]\n",
      " 10%|▉         | 48/484 [03:03<17:12,  2.37s/it]\n",
      " 10%|█         | 49/484 [03:04<15:35,  2.15s/it]\n",
      " 10%|█         | 50/484 [03:06<14:53,  2.06s/it]\n",
      " 11%|█         | 51/484 [03:08<14:40,  2.03s/it]\n",
      " 11%|█         | 52/484 [03:13<19:52,  2.76s/it]\n",
      " 11%|█         | 53/484 [03:17<22:51,  3.18s/it]\n",
      " 11%|█         | 54/484 [03:22<26:13,  3.66s/it]\n",
      " 11%|█▏        | 55/484 [03:26<28:37,  4.00s/it]\n",
      " 12%|█▏        | 56/484 [03:31<30:36,  4.29s/it]\n",
      " 12%|█▏        | 57/484 [03:36<31:33,  4.43s/it]\n",
      " 12%|█▏        | 58/484 [03:41<31:58,  4.50s/it]\n",
      " 12%|█▏        | 59/484 [03:46<32:36,  4.60s/it]\n",
      " 12%|█▏        | 60/484 [03:50<33:01,  4.67s/it]\n",
      " 13%|█▎        | 61/484 [03:55<32:51,  4.66s/it]\n",
      " 13%|█▎        | 62/484 [04:00<32:31,  4.62s/it]\n",
      " 13%|█▎        | 63/484 [04:04<31:36,  4.51s/it]\n",
      " 13%|█▎        | 64/484 [04:08<31:09,  4.45s/it]\n",
      " 13%|█▎        | 65/484 [04:13<30:52,  4.42s/it]\n",
      " 14%|█▎        | 66/484 [04:17<30:30,  4.38s/it]\n",
      " 14%|█▍        | 67/484 [04:18<24:45,  3.56s/it]\n",
      " 14%|█▍        | 68/484 [04:20<21:23,  3.08s/it]\n",
      " 14%|█▍        | 69/484 [04:28<31:17,  4.52s/it]\n",
      " 14%|█▍        | 70/484 [04:36<38:01,  5.51s/it]\n",
      " 15%|█▍        | 71/484 [04:43<41:40,  6.05s/it]\n",
      " 15%|█▍        | 72/484 [04:51<43:59,  6.41s/it]\n",
      " 15%|█▌        | 73/484 [04:58<45:12,  6.60s/it]\n",
      " 15%|█▌        | 74/484 [05:06<47:36,  6.97s/it]\n",
      " 15%|█▌        | 75/484 [05:13<47:49,  7.02s/it]\n",
      " 16%|█▌        | 76/484 [05:20<48:15,  7.10s/it]\n",
      " 16%|█▌        | 77/484 [05:25<44:34,  6.57s/it]\n",
      " 16%|█▌        | 78/484 [05:30<41:36,  6.15s/it]\n",
      " 16%|█▋        | 79/484 [05:36<39:55,  5.91s/it]\n",
      " 17%|█▋        | 80/484 [05:41<38:38,  5.74s/it]\n",
      " 17%|█▋        | 81/484 [05:45<35:38,  5.31s/it]\n",
      " 17%|█▋        | 82/484 [05:50<33:35,  5.01s/it]\n",
      " 17%|█▋        | 83/484 [05:54<31:55,  4.78s/it]\n",
      " 17%|█▋        | 84/484 [05:59<32:11,  4.83s/it]\n",
      " 18%|█▊        | 85/484 [06:01<25:34,  3.85s/it]\n",
      " 18%|█▊        | 86/484 [06:02<20:38,  3.11s/it]\n",
      " 18%|█▊        | 87/484 [06:04<17:35,  2.66s/it]\n",
      " 18%|█▊        | 88/484 [06:05<14:30,  2.20s/it]\n",
      " 18%|█▊        | 89/484 [06:06<12:31,  1.90s/it]\n",
      " 19%|█▊        | 90/484 [06:08<13:56,  2.12s/it]\n",
      " 19%|█▉        | 91/484 [06:11<15:31,  2.37s/it]\n",
      " 19%|█▉        | 92/484 [06:14<16:16,  2.49s/it]\n",
      " 19%|█▉        | 93/484 [06:17<17:10,  2.63s/it]\n",
      " 19%|█▉        | 94/484 [06:20<16:32,  2.54s/it]\n",
      " 20%|█▉        | 95/484 [06:22<16:15,  2.51s/it]\n",
      " 20%|█▉        | 96/484 [06:25<16:44,  2.59s/it]\n",
      " 20%|██        | 97/484 [06:29<19:44,  3.06s/it]\n",
      " 20%|██        | 98/484 [06:33<21:30,  3.34s/it]\n",
      " 20%|██        | 99/484 [06:37<23:01,  3.59s/it]\n",
      " 21%|██        | 100/484 [06:41<23:18,  3.64s/it]\n",
      " 21%|██        | 101/484 [06:45<24:09,  3.78s/it]\n",
      " 21%|██        | 102/484 [06:48<22:10,  3.48s/it]\n",
      " 21%|██▏       | 103/484 [06:50<19:47,  3.12s/it]\n",
      " 21%|██▏       | 104/484 [06:52<18:31,  2.93s/it]\n",
      " 22%|██▏       | 105/484 [06:55<18:31,  2.93s/it]\n",
      " 22%|██▏       | 106/484 [07:00<21:50,  3.47s/it]\n",
      " 22%|██▏       | 107/484 [07:04<23:26,  3.73s/it]\n",
      " 22%|██▏       | 108/484 [07:09<24:44,  3.95s/it]\n",
      " 23%|██▎       | 109/484 [07:13<25:15,  4.04s/it]\n",
      " 23%|██▎       | 110/484 [07:18<26:18,  4.22s/it]\n",
      " 23%|██▎       | 110/484 [07:18<24:50,  3.98s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"exteval/preprocessForCorrected.py\", line 123, in <module>\n",
      "    processed_data = preprocess(data)\n",
      "  File \"exteval/preprocessForCorrected.py\", line 97, in preprocess\n",
      "    res = find_coreference(summary)\n",
      "  File \"exteval/preprocessForCorrected.py\", line 58, in find_coreference\n",
      "    res = predictor.predict(document=document)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/allennlp_models/coref/predictors/coref.py\", line 64, in predict\n",
      "    return self.predict_json({\"document\": document})\n",
      "  File \"/stage/allennlp/allennlp/predictors/predictor.py\", line 54, in predict_json\n",
      "    instance = self._json_to_instance(inputs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/allennlp_models/coref/predictors/coref.py\", line 202, in _json_to_instance\n",
      "    instance = self._dataset_reader.text_to_instance(sentences)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/allennlp_models/coref/dataset_readers/conll.py\", line 96, in text_to_instance\n",
      "    return make_coref_instance(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/allennlp_models/coref/util.py\", line 150, in make_coref_instance\n",
      "    span_field = ListField(spans)\n",
      "  File \"/stage/allennlp/allennlp/data/fields/list_field.py\", line 30, in __init__\n",
      "    assert (\n",
      "AssertionError: ListFields must contain a single field type, found set()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define the input and output files\n",
    "data_file = \"data/corrected/corrected_data_modified.json\"\n",
    "output_file = \"data/corrected/preprocessed/corrected_data_preprocessed_modified.json\"\n",
    "\n",
    "# Build and execute the command\n",
    "command = [\"python\", \"exteval/preprocessForCorrected.py\", \"--data_file\", data_file, \"--output_file\", output_file]\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(command, check=True, text=True, capture_output=True)\n",
    "    print(\"Script executed successfully:\")\n",
    "    print(result.stdout)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Error while executing the script:\")\n",
    "    print(e.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99546ff6-1fd3-4f29-9487-c7f4880650af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define the input and output files\n",
    "data_file = \"data/corrected/preprocessed/corrected_data_preprocessed_modified.json\"\n",
    "output_file = \"data/corrected/scores/corrected_data_exteval_modified.json\"\n",
    "\n",
    "# Build and execute the command\n",
    "command = [\"python\", \"exteval/extevalModifiedForCorrected.py\", \"--data_file\", data_file, \"--output_file\", output_file]\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(command, check=True, text=True, capture_output=True)\n",
    "    print(\"Script executed successfully:\")\n",
    "    print(result.stdout)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Error while executing the script:\")\n",
    "    print(e.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9809c9ed-6c8f-4e98-b3eb-0d7cac4d8040",
   "metadata": {},
   "source": [
    "### Step 6: The new ExtEval-Modified scores are now compared to the old ones to see the improvement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cee12aa-8e7e-4c22-b4e3-5cc50a7aeb17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31de8ee-5731-4a61-80c8-f458a92801c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
